{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Billion Something-Something\n",
    "\n",
    "Script for processing the 20bn dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display video grid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pysymbolic #-e ../../symbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy h5py hdf5plugin Pillow tqdm pandas av seaborn ipywidgets opencv-python 'mediapipe==0.10.0'\n",
    "# !pip install torch torchvision tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from gpred import video_utils\n",
    "from env import twentybn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import symbolic\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.env / \"problem.pddl\"))\n",
    "# pddl = symbolic.Pddl(str(paths.domain_pddl), str(paths.problem_pddl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20BN Something Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SomethingElse  data  labels  videos\n"
     ]
    }
   ],
   "source": [
    "!ls /Something2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -s /Something2/labels ../data/twentybn/labels\n",
    "# !ln -s /Something2/videos ../data/twentybn/videos\n",
    "# !ln -s /Something2/SomethingElse ../data/twentybn/SomethingElse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Reformat labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s20bn_utils.predicate_tests import generate_tests, precompute_tests, process_action\n",
    "from s20bn_utils.build_dataset import append_pre_post_to_dataset\n",
    "from s20bn_utils.build_dataset import build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:37<00:00, 39.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/twentybn\n",
      "../data/twentybn/labels.hdf5 exists\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, video_labels, action_instances = build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = twentybn.dataset.Labels(paths.data / \"labels.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Extract pre and post frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "import time\n",
    "import symbolic\n",
    "from s20bn_utils.precompute_hands import load_hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hands = load_hands()\n",
    "for k in hands:\n",
    "    hands[k] = [x[0] for x in hands[k]]  # only if return [r] in hand_detecotr.py\n",
    "len(hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate pre/post-condition tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from s20bn_utils.build_dataset import append_pre_post_to_dataset\n",
    "from s20bn_utils.predicate_tests import evaluate_pre_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = generate_tests(pddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apps.hand_detector import Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in hands:\n",
    "#     h=hands[i]\n",
    "#     for hi in h:\n",
    "#         print(len(hi), [[len(x) for x in x] for x in hi], hi)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████▍                                                                                         | 43/174 [04:14<13:42,  6.28s/it]/scratch/bs3639/ego2023/grounding-predicates/scripts/s20bn_utils/predicate_tests.py:68: RuntimeWarning: invalid value encountered in divide\n",
      "  v_line /= np.linalg.norm(v_line)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 174/174 [15:50<00:00,  5.46s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/state/partition1/job-43157701/ipykernel_2846710/680303277.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pre_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpddl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/bs3639/ego2023/grounding-predicates/scripts/s20bn_utils/predicate_tests.py:531\u001b[0m, in \u001b[0;36mevaluate_pre_post\u001b[0;34m(pddl, paths, hands, tests, labels)\u001b[0m\n\u001b[1;32m    529\u001b[0m     test_results\u001b[38;5;241m.\u001b[39mupdate(process_action(id_action, hands, tests, labels))\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(paths\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition_test_results.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 531\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(test_results, f)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tests, test_results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = evaluate_pre_post(pddl, paths, hands, tests, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "append_pre_post_to_dataset(test_results, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Compute condition test statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook\n",
    "\n",
    "import config\n",
    "\n",
    "def compute_condition_statistics(paths: config.EnvironmentPaths, train_set: typing.List[int], val_set: typing.List[int]):\n",
    "    \"\"\"Computes pre/post-condition statistics for the 20BN dataset.\n",
    "    \n",
    "    Args:\n",
    "        paths: Environment paths.\n",
    "        train_set: Video ids in the original 20BN train set.\n",
    "        val_set: Video ids in the original 20BN val set.\n",
    "    Returns:\n",
    "    | Video | Action | Dataset | Pre | Post |\n",
    "    \"\"\"\n",
    "    df = {\n",
    "        \"Video\": [],\n",
    "        \"Action\": [],\n",
    "        \"Dataset\": [],\n",
    "        \"Pre\": [],\n",
    "        \"Post\": [],\n",
    "    }\n",
    "    val_set = set(val_set)\n",
    "    with h5py.File(paths.data / \"labels.hdf5\", \"r\") as f:\n",
    "        grp_videos = f[\"videos\"]\n",
    "        video_ids = np.array(f[\"video_ids\"])\n",
    "        for id_video in tqdm.notebook.tqdm(video_ids):\n",
    "            grp_video = grp_videos[str(id_video)]\n",
    "            \n",
    "            id_action = int(grp_video.attrs[\"id_action\"])\n",
    "            \n",
    "            # Assume video is in either train or val set.\n",
    "            dataset = \"val\" if id_video in val_set else \"train\"\n",
    "            \n",
    "            pre = grp_video[\"pre\"].size\n",
    "            post = grp_video[\"post\"].size\n",
    "            \n",
    "            df[\"Video\"].append(id_video)\n",
    "            df[\"Action\"].append(id_action)\n",
    "            df[\"Dataset\"].append(dataset)\n",
    "            df[\"Pre\"].append(pre)\n",
    "            df[\"Post\"].append(post)\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats = compute_condition_statistics(paths, train_set, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train, val, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = generate_dataset_splits(pddl, stats, train_set, val_set)\n",
    "\n",
    "print(f\"Train: {len(train_set)}\")\n",
    "print(f\"Val: {len(val_set)}\")\n",
    "print(f\"Test: {len(test_set)}\")\n",
    "\n",
    "with open(paths.data / \"dataset_splits.pkl\", \"wb\") as f:\n",
    "    pickle.dump((train_set, val_set, test_set), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze condition statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.subplots(figsize=(5, 40))\n",
    "\n",
    "stats[[\"Action\"]] \\\n",
    "    .assign(Pre = stats.Pre > 0, Post = stats.Post > 0) \\\n",
    "    .groupby(\"Action\", as_index=False) \\\n",
    "    .mean() \\\n",
    "    .melt(id_vars=\"Action\", value_vars=[\"Pre\",\"Post\"], var_name=\"Condition\") \\\n",
    "    .pipe((sns.barplot, \"data\"), y=\"Action\", x=\"value\", hue=\"Condition\", orient=\"h\")\n",
    "\n",
    "plt.savefig(\"figures/pre_post.png\", bbox_inches=\"tight\", transparent=\"True\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Partial: {len(stats[(stats.Pre > 0) | (stats.Post > 0)])} / {len(stats)}\")\n",
    "print(f\"Complete: {len(stats[(stats.Pre > 0) & (stats.Post > 0)])} / {len(stats)}\")\n",
    "print(f\"Train: {len(stats[(stats.Dataset == 'train') & (stats.Pre > 0) & (stats.Post > 0)])} / {len(stats[stats.Dataset == 'train'])}\")\n",
    "print(f\"Val: {len(stats[(stats.Dataset == 'val') & (stats.Pre > 0) & (stats.Post > 0)])} / {len(stats[stats.Dataset == 'val'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize condition tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort actions by proportion of videos with pre/post frames (from lowest to highest).\n",
    "\n",
    "id_actions = np.array(stats[[\"Action\"]] \\\n",
    "    .assign(Pre = stats.Pre > 0, Post = stats.Post > 0) \\\n",
    "    .groupby(\"Action\", as_index=False) \\\n",
    "    .mean() \\\n",
    "    .melt(id_vars=\"Action\", value_vars=[\"Pre\",\"Post\"], var_name=\"Condition\") \\\n",
    "    [[\"Action\", \"value\"]] \\\n",
    "    .groupby(\"Action\", as_index=False) \\\n",
    "    .min() \\\n",
    "    .sort_values(\"value\") \\\n",
    "    [[\"Action\"]]).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate videos for 30 worst-performing actions.\n",
    "\n",
    "\" \".join(str(id_action) for id_action in id_actions[:30])\n",
    "\n",
    "for id_action in tqdm.notebook.tqdm(id_actions[:30]):\n",
    "    process_action(id_action, generate_video=True, num_videos=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from s20bn_utils.draw import display_video_grid\n",
    "\n",
    "path_videos = paths.data / \"labeled_videos\"\n",
    "id_videos = [int(p.stem) for p in path_videos.iterdir() if p.suffix in {\".mp4\", \".webm\"}]\n",
    "\n",
    "mini_action_instances = [[] for _ in range(len(pddl.actions))]\n",
    "for id_video in id_videos:\n",
    "    id_action = labels.videos[id_video].id_action\n",
    "    mini_action_instances[id_action].append(id_video)\n",
    "\n",
    "display_video_grid(labels, mini_action_instances, paths.data / \"labeled_videos\", num_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_action = 60\n",
    "test_results.update(process_action(id_action))\n",
    "with open(paths.data / \"condition_test_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_results, f)\n",
    "\n",
    "append_pre_post_to_dataset(test_results, paths, id_action=id_action)\n",
    "_ = process_action(id_action, generate_video=True, num_videos=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_video = 43454\n",
    "find_pre_post_frames(test_results[id_video])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "\"\"\"\n",
    "action_labels = [\n",
    "    {\n",
    "        \"label\": \"Approaching something with your camera\",\n",
    "        \"template\": \"Approaching [something] with your camera\",\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "with open(paths.data / \"action_labels.pkl\", \"rb\") as f:\n",
    "    action_labels = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "action_instances = [\n",
    "    [{id_video}, ...]\n",
    "]\n",
    "\"\"\"\n",
    "with open(paths.data / \"action_instances.pkl\", \"rb\") as f:\n",
    "    action_instances = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "video_labels = {\n",
    "    {id_video}: {\n",
    "        \"id_action\": id_action,\n",
    "        \"placeholders\": [\"a potato\", \"a vicks vaporub bottle\"],\n",
    "        \"objects\": [\"potato\", \"bottle\"],\n",
    "        \"frames\": {\n",
    "            idx_frame: {\n",
    "                \"{id_object/hand}\": [[x1, y1], [x2, y2]],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "with open(paths.data / \"video_labels.pkl\", \"rb\") as f:\n",
    "    video_labels = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "train_set = [{id_video}, ...]\n",
    "\"\"\"\n",
    "with open(paths.data / \"train_set.pkl\", \"rb\") as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "val_set = [{id_video}, ...]\n",
    "\"\"\"\n",
    "with open(paths.data / \"val_set.pkl\", \"rb\") as f:\n",
    "    val_set = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "video_ranges = {\n",
    "    {id_video}: (\n",
    "        [idx_pre_frames, ...],\n",
    "        [idx_post_frames, ...]\n",
    "    )\n",
    "}\n",
    "\"\"\"\n",
    "with open(paths.data / \"video_ranges.pkl\", \"rb\") as f:\n",
    "    video_ranges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate hdf5 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from s20bn_utils.build_dataset import extract_pre_post\n",
    "\n",
    "#extract_pre_post(train_set[:10000], \"pre_post_train_mini\", paths.data)\n",
    "#extract_pre_post(val_set[:10000], \"pre_post_val_mini\", paths.data)\n",
    "extract_pre_post(train_set, \"pre_post_train\", paths.data)\n",
    "extract_pre_post(val_set, \"pre_post_val\", paths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s20bn_utils.build_dataset import create_predicate_dataset\n",
    "create_predicate_dataset(pddl, labels, train_set, \"predicate_train\", paths.data)\n",
    "create_predicate_dataset(pddl, labels, val_set, \"predicate_val\", paths.data)\n",
    "create_predicate_dataset(pddl, labels, test_set, \"predicate_test\", paths.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analyze dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "from gpred import dnf_utils\n",
    "\n",
    "\n",
    "def plot_predicate_counts(stats: pd.DataFrame):\n",
    "    \"\"\"Plots predicates (x) vs. count (y).\n",
    "    \n",
    "    Args:\n",
    "        stats: Longform dataframe output by `compute_pddl_statistics()`.\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    g = sns.countplot(data=stats.sort_values(\"Predicate\"), x=\"Predicate\", hue=\"Label\")\n",
    "    for item in g.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "\n",
    "def plot_dnfs(stats: pd.DataFrame):\n",
    "    \"\"\"Plots a heatmap of actions vs. propositions specified by their DNFs.\n",
    "    \n",
    "    Args:\n",
    "        stats: Longform table output by compute_pddl_statistics().\n",
    "    \"\"\"\n",
    "    SIZE_SECTION = 10\n",
    "    CMAP = sns.diverging_palette(10, 130, n=100)\n",
    "    \n",
    "    df_action_v_prop = stats.astype({\"Label\": float}).pivot(index=[\"Action\", \"Condition\"], columns=\"Proposition\", values=\"Label\")\n",
    "    num_rows = len(df_action_v_prop)\n",
    "    num_sections = math.ceil(num_rows / SIZE_SECTION)\n",
    "\n",
    "    f, axs = plt.subplots(num_sections, 1, figsize=(25, num_sections * 5))\n",
    "\n",
    "    for i in tqdm.notebook.tqdm(range(num_sections)):\n",
    "        plt.subplot(num_sections, 1, i + 1)\n",
    "        g = sns.heatmap(data=df_action_v_prop[i*SIZE_SECTION:min(len(df_action_v_prop), (i+1)*SIZE_SECTION)], square=True, cmap=CMAP, linewidths=0.5, linecolor=\"#eee\", cbar_kws={\"shrink\": 0.5})\n",
    "        \n",
    "def plot_predicate_weights(w: np.ndarray):\n",
    "    \"\"\"Plots predicates (x) vs. weight (y).\n",
    "    \n",
    "    Args:\n",
    "        stats: Longform dataframe output by `compute_pddl_statistics()`.\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    df = pd.DataFrame(w.T, columns=[\"Pos\", \"Neg\"], index=[str(pred) for pred in pddl.predicates])\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df = pd.melt(df, id_vars=[\"index\"], value_vars=[\"Pos\",\"Neg\"])\n",
    "    df = df.rename(columns={\"index\": \"Predicate\", \"variable\": \"Label\", \"value\": \"Weight\"})\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    g = sns.barplot(data=df.sort_values(\"Predicate\"), x=\"Predicate\", y=\"Weight\", hue=\"Label\")\n",
    "    for item in g.get_xticklabels():\n",
    "        item.set_rotation(90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(paths.data / \"predicate_val.hdf5\",\"r\") as f:\n",
    "    actions = [str(action) for action in pddl.actions]\n",
    "    action_instances = [actions[idx_action] for idx_action in f[\"actions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dnf_utils.compute_pddl_statistics(pddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.problem_pddl))\n",
    "stats = dnf_utils.compute_pddl_statistics(pddl)\n",
    "\n",
    "plot_predicate_counts(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = dnf_utils.compute_predicate_class_weights(pddl, action_instances=action_instances)\n",
    "plot_predicate_weights(np.minimum(1, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_inv = dnf_utils.compute_predicate_class_weights(pddl)\n",
    "plot_predicate_weights(w_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dnfs(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Find video resolution ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(paths.data / \"pre_post_train.hdf5\", \"r\") as f:\n",
    "    H_max = 0\n",
    "    H_min = 10000\n",
    "    W_max = 0\n",
    "    W_min = 10000\n",
    "    for id_video in tqdm.tqdm(f[\"videos\"]):\n",
    "        dim = f[str(id_video)][\"images\"].shape[2:]\n",
    "        H_min = min(H_min, dim[0])\n",
    "        H_max = max(H_max, dim[0])\n",
    "        W_min = min(W_min, dim[1])\n",
    "        W_max = max(W_max, dim[1])\n",
    "\n",
    "with h5py.File(paths.data / \"pre_post_val.hdf5\", \"r\") as f:\n",
    "    for id_video in tqdm.tqdm(f[\"videos\"]):\n",
    "        dim = f[str(id_video)][\"images\"].shape[2:]\n",
    "        H_min = min(H_min, dim[0])\n",
    "        H_max = max(H_max, dim[0])\n",
    "        W_min = min(W_min, dim[1])\n",
    "        W_max = max(W_max, dim[1])\n",
    "\n",
    "print(H_max, W_max, H_min, W_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### List videos with mismatching placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_video, video_label in video_labels.items():\n",
    "    if len(video_label[\"objects\"]) != len(video_label[\"placeholders\"]):\n",
    "        if not video_label[\"id_action\"] in (102, 144):\n",
    "            print(id_video, video_label[\"id_action\"], video_label[\"objects\"], video_label[\"placeholders\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
